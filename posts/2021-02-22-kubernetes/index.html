<!DOCTYPE html>
<html lang="en-us">
<head>
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script type="text/javascript" src="https://latest.cactus.chat/cactus.js"></script>
  <link rel="stylesheet" href="https://latest.cactus.chat/style.css" type="text/css">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> Kubernetes | अहो अहं </title>
  <link rel = 'canonical' href = 'https://uzak.github.io/posts/2021-02-22-kubernetes/'>
  <meta name="description" content="Welcome! I&#39;m Martin and I work as IT mentor.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:title" content="Kubernetes" />
<meta property="og:description" content="source
Install and Deploy  install kubectl install minikube  Commands:
minikube start minikube status kubectl get node kubectl get pods kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=NodePort --port=8080 minikube service hello-minikube --url curl http://192.168.49.2:31223 kubectl delete deployments.apps hello-minikube minikube pause minikube unpause minikube stop  Namespaces  for organization and resource separation kubectl --namespace=mystuff or kubectl -n=mystuff kubectl --all-namespaces  Default namespaces for new clusters:
$ kubectl get ns NAME STATUS AGE default Active 13m # k8s resources are crated here by default kube-node-lease Active 13m # storage for node lease information kube-public Active 13m # world-readable kube-system Active 13m # infrastructure pods  Commands:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://uzak.github.io/posts/2021-02-22-kubernetes/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-02-22T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-02-22T00:00:00+00:00" />


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Kubernetes"/>
<meta name="twitter:description" content="source
Install and Deploy  install kubectl install minikube  Commands:
minikube start minikube status kubectl get node kubectl get pods kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=NodePort --port=8080 minikube service hello-minikube --url curl http://192.168.49.2:31223 kubectl delete deployments.apps hello-minikube minikube pause minikube unpause minikube stop  Namespaces  for organization and resource separation kubectl --namespace=mystuff or kubectl -n=mystuff kubectl --all-namespaces  Default namespaces for new clusters:
$ kubectl get ns NAME STATUS AGE default Active 13m # k8s resources are crated here by default kube-node-lease Active 13m # storage for node lease information kube-public Active 13m # world-readable kube-system Active 13m # infrastructure pods  Commands:"/>

  
  
    
  
  
  <link rel="stylesheet" href="https://uzak.github.io/css/styles.94f653e9e151e28067a7c5dbbc4600cbd5a3c721e79faaf971e523c40f3b249b8e4f20bb57810dfffa8d559ca5c140fd56eb4cd9c0853113ad08e66afdb08bdd.css" integrity="sha512-lPZT6eFR4oBnp8XbvEYAy9WjxyHnn6r5ceUjxA87JJuOTyC7V4EN//qNVZylwUD9VutM2cCFMROtCOZq/bCL3Q=="> 

  
  
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="https://uzak.github.io/images/favicon.ico" />

  
  
  
  
</head>

<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

  <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;" aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/posts">Blog</a></li>
         
        <li><a href="/tags">Tags</a></li>
         
        <li><a href="/etymolog">Etymolog</a></li>
         
        <li><a href="/sask.pdf">SA-SK</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li>
          <a class="icon" href=" https://uzak.github.io/posts/2021-01-18-rudram/" aria-label="Previous">
            <i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i>
          </a>
        </li>
        
        
        <li>
          <a class="icon" href="https://uzak.github.io/posts/2021-03-01-prusa_link_architecture/" aria-label="Next">
            <i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i>
          </a>
        </li>
        
        <li>
          <a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" aria-label="Top of Page">
            <i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i>
          </a>
        </li>
        <li>
          <a class="icon" href="#" aria-label="Share">
            <i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i>
          </a>
        </li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f" aria-label="Facebook">
      <i class="fab fa-facebook " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f&text=Kubernetes" aria-label="Twitter">
      <i class="fab fa-twitter " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f&title=Kubernetes" aria-label="Linkedin">
      <i class="fab fa-linkedin " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f&is_video=false&description=Kubernetes" aria-label="Pinterest">
      <i class="fab fa-pinterest " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=Kubernetes&body=Check out this article: https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f" aria-label="Email">
      <i class="fas fa-envelope " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f&title=Kubernetes" aria-label="Pocket">
      <i class="fab fa-get-pocket " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f&title=Kubernetes" aria-label="reddit">
      <i class="fab fa-reddit " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f&name=Kubernetes&description=source%0aInstall%20and%20Deploy%20%20install%20kubectl%20install%20minikube%20%20Commands%3a%0aminikube%20start%20minikube%20status%20kubectl%20get%20node%20kubectl%20get%20pods%20kubectl%20create%20deployment%20hello-minikube%20--image%3dk8s.gcr.io%2fechoserver%3a1.4%20kubectl%20expose%20deployment%20hello-minikube%20--type%3dNodePort%20--port%3d8080%20minikube%20service%20hello-minikube%20--url%20curl%20http%3a%2f%2f192.168.49.2%3a31223%20kubectl%20delete%20deployments.apps%20hello-minikube%20minikube%20pause%20minikube%20unpause%20minikube%20stop%20%20Namespaces%20%20for%20organization%20and%20resource%20separation%20kubectl%20--namespace%3dmystuff%20or%20kubectl%20-n%3dmystuff%20kubectl%20--all-namespaces%20%20Default%20namespaces%20for%20new%20clusters%3a%0a%24%20kubectl%20get%20ns%20NAME%20STATUS%20AGE%20default%20Active%2013m%20%23%20k8s%20resources%20are%20crated%20here%20by%20default%20kube-node-lease%20Active%2013m%20%23%20storage%20for%20node%20lease%20information%20kube-public%20Active%2013m%20%23%20world-readable%20kube-system%20Active%2013m%20%23%20infrastructure%20pods%20%20Commands%3a" aria-label="Tumblr">
      <i class="fab fa-tumblr " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f&t=Kubernetes" aria-label="Hacker News">
      <i class="fab fa-hacker-news " aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>
    
  </span>
</div>


  <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
    <header>
      <h1 class="posttitle" itemprop="name headline">
        Kubernetes
      </h1>
      <div class="meta">
        
        <div class="postdate">
          
          <time datetime="2021-02-22 00:00:00 &#43;0000 UTC" itemprop="datePublished">2021-02-22</time>
          
        </div>
        
        
        <div class="article-read-time">
          <i class="far fa-clock"></i>
          
          21 minute read
        </div>
        
        
        
        <div class="article-tag">
            <i class="fas fa-tag"></i>
            
            
            <a class="tag-link" href="/tags/it" rel="tag">it</a>
            
        </div>
        
      </div>
    </header>

  
    
    <div id="toc">
      <h2>Table of Contents</h2>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#install-and-deploy">Install and Deploy</a></li>
    <li><a href="#namespaces">Namespaces</a></li>
    <li><a href="#pods">Pods</a>
      <ul>
        <li><a href="#multi-container-pods-use-cases">multi-container Pods use-cases</a></li>
        <li><a href="#jobs">Jobs</a></li>
      </ul>
    </li>
    <li><a href="#init-containers">Init containers</a></li>
    <li><a href="#rolling-updates">Rolling Updates</a></li>
    <li><a href="#labels">Labels</a></li>
    <li><a href="#replication-controller-and-replica-sets">Replication Controller and Replica Sets</a>
      <ul>
        <li><a href="#replicasets">ReplicaSets</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-services">Kubernetes Services</a></li>
    <li><a href="#daemonsets">DaemonSets</a></li>
    <li><a href="#scheduling-jobs">Scheduling Jobs</a></li>
    <li><a href="#volumes">Volumes</a>
      <ul>
        <li><a href="#nfs">NFS</a></li>
        <li><a href="#persistent-volumes">Persistent Volumes</a>
          <ul>
            <li><a href="#persistentvolumeclaim">PersistentVolumeClaim</a></li>
          </ul>
        </li>
        <li><a href="#configmaps">ConfigMaps</a></li>
        <li><a href="#kubernetes-secrets">Kubernetes Secrets</a></li>
      </ul>
    </li>
    <li><a href="#stateful-sets">Stateful sets</a></li>
    <li><a href="#k8s-api-server">K8s API Server</a>
      <ul>
        <li><a href="#service-account--roles">Service Account + Roles</a></li>
        <li><a href="#containers-security-context">Container&rsquo;s Security Context</a></li>
        <li><a href="#configure-containers-security-context">Configure container&rsquo;s security context</a></li>
        <li><a href="#individual-capabilities">Individual Capabilities</a></li>
      </ul>
    </li>
    <li><a href="#authentication">Authentication</a></li>
    <li><a href="#authorization">Authorization</a>
      <ul>
        <li><a href="#rbac">RBAC</a></li>
      </ul>
    </li>
    <li><a href="#limit-resources">Limit Resources</a>
      <ul>
        <li><a href="#resource-quota-types">Resource quota types</a></li>
        <li><a href="#limit-range">Limit Range</a></li>
        <li><a href="#limiting-resources">Limiting resources</a></li>
      </ul>
    </li>
    <li><a href="#expose-containers-to-external-networks">Expose containers to external networks</a>
      <ul>
        <li><a href="#kubectl-port-forwarding">kubectl port forwarding</a></li>
        <li><a href="#kubectl-expose">kubectl expose</a></li>
        <li><a href="#ingress">Ingress</a>
          <ul>
            <li><a href="#configure-ingress-using-host">Configure Ingress using Host</a></li>
            <li><a href="#configure-ingress-using-path">Configure Ingress using Path</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
    
    <div class="content" itemprop="articleBody">
      <p><a href="https://www.golinuxcloud.com/kubernetes-tutorial/">source</a></p>
<h1 id="install-and-deploy">Install and Deploy</h1>
<ul>
<li>install kubectl</li>
<li>install minikube</li>
</ul>
<p>Commands:</p>
<pre><code>minikube start  
minikube status

kubectl get node
kubectl get pods

kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4
kubectl expose deployment hello-minikube --type=NodePort --port=8080
minikube service hello-minikube --url
curl http://192.168.49.2:31223
kubectl delete deployments.apps hello-minikube 

minikube pause
minikube unpause
minikube stop
</code></pre>
<h1 id="namespaces">Namespaces</h1>
<ul>
<li>for organization and resource separation</li>
<li><code>kubectl --namespace=mystuff</code> or <code>kubectl -n=mystuff</code></li>
<li><code>kubectl --all-namespaces</code></li>
</ul>
<p>Default namespaces for new clusters:</p>
<pre><code>$ kubectl get ns  
NAME              STATUS   AGE
default           Active   13m      # k8s resources are crated here by default
kube-node-lease   Active   13m      # storage for node lease information
kube-public       Active   13m      # world-readable
kube-system       Active   13m      # infrastructure pods
</code></pre>
<p>Commands:</p>
<pre><code>kubectl get all --all-namespaces    # all objects in all ns
kubectl get pods -n default         # all pods in default ns
kubectl api-resources | grep -iE 'namespace|KIND'  # get res name: Namespace
kubectl explain Namespace | head -n 2              # get its version

# create app-ns.yml
kubectl create -f app-ns.yml 
kubectl create ns dev
kubectl get ns
kubectl describe ns dev
kubectl get ns default -o yaml

# create nginx-app.yml
kubectl create -f nginx-app.yml

kubectl get pods -n default nginx-app   # not found
kubectl get pods -n app nginx-app       # found

kubectl delete ns dev                       # delete everything in a ns incl. 
kubectl delete pods -n app --all            # keep namespace, delete all pods
kubectl delete all -n app --all             # delete all
</code></pre>
<h1 id="pods">Pods</h1>
<ul>
<li>declarative way: uses yaml files, like nginx-app.yml above</li>
<li>imperative way: uses <code>kubectl</code></li>
<li>a pod contains containers. Usually one. If a pod has more than one containers, they are always executed on a single worker, never span to multiple workers.</li>
</ul>
<p>Examples:</p>
<pre><code>kubectl create -f nginx.yml
kubectl get pods -o wide
kubectl describe pod nginx          # shows the node
docker ps                           # get status on the node
kubectl exec -it nginx -c nginx -- /bin/bash    

kubectl port-forward nginx 8000:80 
</code></pre>
<h2 id="multi-container-pods-use-cases">multi-container Pods use-cases</h2>
<ul>
<li><strong>sidecar container</strong>: enhances primary app, e.g. logging. Both are closely related.</li>
<li>ambassador container: a container represents a primary container for outside e.g. proxy</li>
<li>adapter container: to adapt/normalize traffic for other apps inside</li>
</ul>
<p>Examples:</p>
<pre><code># create create-sidecar.yml
kubectl create -f create-sidecar.yml 
kubectl get pods -o wide

$ kubectl exec -it sidecar-pod -c sidecar -- /bin/bash
[root@sidecar-pod /]# curl http://localhost/date.txt
Sun Feb 21 08:53:05 UTC 2021
Sun Feb 21 08:53:15 UTC 2021
Sun Feb 21 08:53:25 UTC 2021
Sun Feb 21 08:53:35 UTC 2021
Sun Feb 21 08:53:45 UTC 2021
Sun Feb 21 08:53:55 UTC 2021
Sun Feb 21 08:54:05 UTC 2021
Sun Feb 21 08:54:15 UTC 2021
Sun Feb 21 08:54:25 UTC 2021
Sun Feb 21 08:54:35 UTC 2021
Sun Feb 21 08:54:45 UTC 2021
[root@sidecar-pod /]# 

kubectl logs sidecar-pod sidecar    # show logs
kubectl delete pod nginx
</code></pre>
<h2 id="jobs">Jobs</h2>
<pre><code>completitions=m         # stop after `m`
parallelism=n           # `n` jobs are started
</code></pre>
<p>Examples:</p>
<pre><code>kubectl explain Job

# create k8s/pod-simple-job.yml
kubectl create -f k8s/pod-simple-job.yml
kubectl get pods        # each execution will add one more pod
kubectl get jobs        
kubectl delete job pod-simple-job
</code></pre>
<p><code>.spec.activeDeadlineSeconds</code> to set an execution deadline no matter how many pods are created.</p>
<h1 id="init-containers">Init containers</h1>
<ul>
<li>run in the same pod as main container</li>
<li>to complete a task before the regular container is started</li>
<li>if a pod restarts, all init containers are executed again</li>
</ul>
<p>Example:</p>
<pre><code>$ cat pod-init-container.yml
...

$ k create -f pod-init-container.yml 

$ k get pods
NAME                       READY   STATUS     RESTARTS   AGE
init-container-example-1   0/1     Init:0/1   0          58s
</code></pre>
<h1 id="rolling-updates">Rolling Updates</h1>
<ul>
<li>A <code>Replica Set</code> manages pods. A <code>Deployment</code> manages a <code>Replica Set</code>.</li>
<li>property: <code>minReadySeconds</code> how long the pod should be ready before it is treated as available. Until it is available, the rollout will not continue.</li>
<li><code>maxSurge</code> - max number of pods that can be created over the desired  number of <code>ReplicaSet</code> before updating</li>
<li><code>maxUnavailable</code> - max number of pods that can be unavailable during rollout</li>
</ul>
<p>Examples:</p>
<pre><code>k create deployment nginx-deploy --image=nginx --dry-run=client -o yaml &gt; nginx-deploy.yml
# adapt nginx-deploy.yml

k create -f nginx-deploy.yml
k get deployments
k get rs            # replica set

# rolling deployment example
k create -f rolling-nginx.yml
k get pods
k get deployments
k get event --field-selector involvedObject.name=rolling-nginx-74cf96d8bb-bn9jq     # filter for pecific pod

k rollout history deployment rolling-nginx  # show rollout history (no record)

k delete deployments.apps rolling-nginx 

k create -f rolling-nginx.yml --record                          # add change cause to history
k set image deployment rolling-nginx nginx=nginx:1.15 --record  # changes will be recorded

kubectl set image deployment rolling-nginx nginx=nginx:1.15 --record

k rollout status deployment rolling-nginx                       # status of deployment

# pause and resume deployment
$ k set image deployment rolling-nginx nginx=nginx:1.16 --record
$ k rollout pause deployment rolling-nginx
$ k rollout status deployment rolling-nginx
Waiting for deployment &quot;rolling-nginx&quot; rollout to finish: 2 out of 4 new replicas have been updated...
$ k get pods -l app=rolling-nginx
NAME                             READY   STATUS              RESTARTS   AGE
rolling-nginx-74cf96d8bb-4pbs2   1/1     Running             0          88s
rolling-nginx-74cf96d8bb-8t2gf   1/1     Running             0          90s
rolling-nginx-74cf96d8bb-jklcv   1/1     Running             0          90s
rolling-nginx-765c4fc67d-dfqgs   0/1     ContainerCreating   0          20s
rolling-nginx-765c4fc67d-nm5sr   0/1     ContainerCreating   0          20s
$ kubectl rollout resume deployment rolling-nginx

# rollback
$ k rollout undo deployment rolling-nginx --to-revision=2
</code></pre>
<h1 id="labels">Labels</h1>
<ul>
<li><strong>key: value</strong> pairs for categorization.</li>
<li>Annotations are labels that won&rsquo;t need to be queried against.</li>
</ul>
<p>Selectors:</p>
<pre><code>tier = frontend
tier != frontend
tier != frontend, game = super-shooter-2

environment in (production, qa)
tier notin (frontend, backend)
partition  # all pods that have partition label - no mater what value

env in (prod, qa), tier notin (fe, be), partition   # joining
</code></pre>
<p>kubectl examples:</p>
<pre><code># Assign labels while creating new objects
k create deployment label-nginx-example --image=nginx --dry-run=client -o yaml &gt; label-nginx-example.yml
# edit the file
k create -f label-nginx-example.yml 
k get deployments --show-labels
k get pods --show-labels 

# Assign new label to existing pod runtime as patch
k patch deployment label-nginx-example --patch &quot;$(cat update-label.yml)&quot;
k describe deployment label-nginx-example
k get pods --show-labels 

# Assign a new label to existing deployments using kubectl
k create -f nginx-deploy.yml 
k get deployments.apps --show-labels 
k label deployment nginx-deploy tier=backend
k get deployments.apps --show-labels    # `tier` added for nginx-deploy

# List resource objects
k get pods --selector 'app=prod'
k get pods --selector 'app=dev' 
k get deployment --selector &quot;app in (prod, dev)&quot;

# Removing labels
k label deployment nginx-deploy type-
k get deployments.apps --show-labels             
</code></pre>
<h1 id="replication-controller-and-replica-sets">Replication Controller and Replica Sets</h1>
<ul>
<li>A replication controller is a k8s resource that ensures that its pods are always running.</li>
<li><strong>It ensures that exact number of pods always matches its label selector.</strong></li>
<li>If a Node is out of resources for creating new pods, it will automatically create new one on another available cluster node.</li>
</ul>
<p>Examples:</p>
<pre><code># create replication-controller.yml
kubectl api-resources | grep -iE 'KIND|replication'
kubectl explain ReplicationController | head -n2    # find out version
kubectl create -f replication-controller.yml 
kubectl get pods            # three new pods
kubectl get rc              # status and list of available rc

k delete pod myapp-rc-mgztn
k get pods                  # one is new
k get pods -o wide
k describe rc myapp-rc

# changing the pod template will affect only newly created pods
k edit rc myapp-rc      # reduce replicas to 2
k get pods              # one is terminating
k get rc                # display status

k scale rc myapp-rc --replicas=6        # horizontal scaling

k delete rc myapp-rc --cascade=false    # to keep its pods running
</code></pre>
<h2 id="replicasets">ReplicaSets</h2>
<ul>
<li>ReplicaSet deprecates ReplicationController. It has more expressive pod selectors. It can match pods lacking a certain selector or just having a key regardless of the value.</li>
<li>Pods aren&rsquo;t owned by RC/RS and can be moved between them if necessary.</li>
</ul>
<p>Examples:</p>
<pre><code>k api-resources | grep -iE 'KIND|replica'
k explain ReplicaSet | head -n 2

k apply -f replica-set.yml      # to manage orphaned pods `app=myapp`
k get rs
k describe rs myapp-replicaset

k delete rs myapp-replicaset
k apply -f replica-set2.yml 
kubectl scale rs myapp-replicaset --replicas=6
k delete rs myapp-replicaset
</code></pre>
<h1 id="kubernetes-services">Kubernetes Services</h1>
<ul>
<li>A Kubernetes Service provides a single, constant (pod are ephemeral) entry point to a group of pods providing the same service.</li>
<li>The kube-proxy agent on the nodes watches the k8s API for new services and endpoints.</li>
</ul>
<h1 id="daemonsets">DaemonSets</h1>
<ul>
<li>A DaemonSet ensures that a Pod is running across a set of nodes.</li>
<li>Deploy system daemons like log collectors and monitoring agents.</li>
</ul>
<p>Examples:</p>
<pre><code>kubectl api-resources | grep -iE 'KIND|daemonse'
kubectl explain DaemonSet | head -n 2

k create -f fluentd-daemonset.yml
k get ds            # inspect daemonsets
k get pods -o wide

# deploy on specific nodes only (label `sdd: true`)
k create -f nginx-daemonset.yml 
k get ds                        # desired = 0
k get nodes --show-labels
k label nodes minikube ssd=true
k get ds                        # desired = 1
k label nodes minikube ssd-     # remove label

k get ds
k delete ds nginx-fast-storage fluentd   # use --cascade to keep pods
</code></pre>
<h1 id="scheduling-jobs">Scheduling Jobs</h1>
<pre><code>kubectl api-resources | grep -iE 'KIND|cron'
kubectl explain cronjob.spec
kubectl explain cronjob.spec.schedule   # alternative

# create pod-cronjob.yml
k create -f pod-cronjob.yml

k get cronjobs.batch        # list avail. jobs
k get jobs --watch          # monitor status
k get pods                  # show completed pods

k delete cronjobs.batch pod-cronjob 
</code></pre>
<h1 id="volumes">Volumes</h1>
<ul>
<li>pods have isolated FS.</li>
<li>Storage volumes aren&rsquo;t top level resources like pods, but are defined a s components of the pod.</li>
<li>Aren&rsquo;t standalone kubernetes objects and cannot be created or deleted on their own.</li>
<li>Volumes live with a Pod across a container life cycle.</li>
</ul>
<p>Types of volumes (Volume Type: Storage Provider):</p>
<ul>
<li><strong>emptyDir</strong>: Localhost. Simplest volume type. Will be ereased, when the Pod is removed.</li>
<li>hostPath: Localhost</li>
<li>glusterfs: GlusterFS cluster</li>
<li>downwardAPI: Kubernets Pod Information</li>
<li><strong>nfs</strong>: NFS server</li>
<li>awsElasticBlockStore: AWS Elastic Block Store</li>
<li>gcePersistentDsik: Google Compute Engine persistent disk</li>
<li>azureDisk: Azure disk storage</li>
<li>projected: Kubernetes resources; currently: secret, downwardAPI and configMap</li>
<li>secret: K8s secret resource</li>
<li>vSphereVolume: vSphere VMDK volume</li>
<li>gitRepo: git repository (volume content will be deleted when Pod is removed)</li>
</ul>
<p>Once you define volumes in <code>volumes</code> section, you can start using them in the <code>volumeMounts</code> section.</p>
<p>Examples:</p>
<pre><code>k create -f shared-volume-emptyDir.yml
k get pods shared-volume-emptydir 
k get pod shared-volume-emptydir -o json

k exec -it shared-volume-emptydir -c alpine1 -- touch /alpine1/someFile.txt
k exec -it shared-volume-emptydir -c alpine2 -- ls -l /alpine2
</code></pre>
<p>To create the dir in memory using tmpfs:</p>
<pre tabindex="0"><code>volumes:
- name: data
- emptydir:
    medium: Memory
</code></pre><p>This gives instead:</p>
<pre><code>$ k exec -it shared-volume-memory -c alpine2 -- df -h /alpine2
Filesystem                Size      Used Available Use% Mounted on
tmpfs                     1.4G         0      1.4G   0% /alpine2
</code></pre>
<p>Instead of original:</p>
<pre><code>$ k exec -it shared-volume-memory -c alpine2 -- df -h /alpine2
/dev/mapper/rhel-root
                     10.3G      7.9G      1.8G  81% /alpine2
</code></pre>
<p><code>hostPath</code> is the first type of persistent storage. Its contents are stored on a node&rsquo;s FS. It is not a good idea to use <code>hostPath</code> for regular poth, because it makes the app sensitive to pod scheduling. Examples:</p>
<pre><code>k create -f shared-volume-hostpath.yml 
k get pods shared-volume-hostpath -o wide   # get the node IP
k exec -it shared-volume-hostpath -c alpine1 -- touch /alpine1/someFile.txt      

[root@nodeIP]# ls -l /tmp/data/
total 0
-rw-r--r-- 1 root root 0 Jan  7 15:26 someFile.txt
</code></pre>
<h2 id="nfs">NFS</h2>
<p>First make sure that <code>nfs-utils</code> package is installed on k8s minions. Check <code>/etc/exports</code> and make sure it can be mounted using <code>nfs -t nfs server:share mountpoint</code>. Examples:</p>
<p>kubectl create -f shared-volume-nfs.yml
kubectl get pods shared-volume-nfs -o wide  # wait for <code>Running</code> state
kubectl describe pod <!-- raw HTML omitted -->     # check mounting status</p>
<p>K8s actually mounts <code>server:share </code> into <code>/var/lib/kubelet/pods/&lt;id&gt;/volumes/kubernetes.io-nfs/nfs</code> and then mounts it into the container as the destination <code>/&lt;mount-point&gt;</code>.</p>
<h2 id="persistent-volumes">Persistent Volumes</h2>
<p>Capacity:</p>
<ul>
<li>K (kilobyte: 1000 bytes)</li>
<li>Ki (kibibyte: 1024 bytes)</li>
</ul>
<p>Volume mode is either <code>Filesystem</code> (default) or <code>Block</code>.</p>
<p>Access Modes:</p>
<ul>
<li>RWO - <strong>ReadWriteOnce</strong> - a single node can mount for RW</li>
<li>ROX - <strong>ReadOnlyMany</strong></li>
<li>RWX - <strong>ReadWriteMany</strong></li>
</ul>
<p>Storage is mounted to nodes, so even with RWO, multiple pods on the same node can mount the volume and write to it.</p>
<p>Reclaim Policy determines, what happens when a persistent volume claim is deleted:</p>
<ul>
<li><strong>Retain</strong>: volume will need to be reclaimed manualy</li>
<li><strong>Delete</strong>: associated storage asset, such as AWS EBS, AzureDisk, OpenStack Cinder volume etc. is deleted.</li>
<li><strong>Recycle</strong>: delete content only. Allows the volume to be claimed again.</li>
</ul>
<p>ATM Only NFS and <code>HostPath</code> support recycling. AWS EBS, GCE PD, Azure Disk and Cinder volumes support deletion.</p>
<p>Example:</p>
<pre><code>k api-resources | grep -iE 'KIND|persistent'
k explain PersistentVolume | head -n 2

k create - f persistent-volume.yml

# kubectl get pv
NAME           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
nfs-share-pv   1Gi        ROX,RWX        Recycle          Available                                   18s
</code></pre>
<p><code>PersistentVolumes</code> don&rsquo;t belong to any namespace. They are cluster-level resources like nodes.</p>
<h3 id="persistentvolumeclaim">PersistentVolumeClaim</h3>
<p>Claiming a <code>PersistentVolume</code> is completely separate process from creating a pod.</p>
<p>Example:</p>
<pre><code>k explain PersistentVolumeClaim | head -n 2

[root@controller ~]# kubectl get pvc
NAME            STATUS   VOLUME         CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nfs-share-pvc   Bound    nfs-share-pv   1Gi        ROX,RWX   

[root@controller ~]# kubectl get pv
NAME           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS   REASON   AGE
nfs-share-pv   1Gi        ROX,RWX        Recycle          Bound    default/nfs-share-pvc                           7m26s

[root@controller ~]# kubectl describe pvc nfs-share-pvc
Name:          nfs-share-pvc
Namespace:     default
StorageClass:
Status:        Bound
Volume:        nfs-share-pv
Labels:        &lt;none&gt;
Annotations:   pv.kubernetes.io/bind-completed: yes
            pv.kubernetes.io/bound-by-controller: yes
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      1Gi
Access Modes:  ROX,RWX
VolumeMode:    Filesystem
Mounted By:    &lt;none&gt;
Events:        &lt;none&gt;
</code></pre>
<p>PV is now yours to use. Nobody else can claim it until you release it.</p>
<h4 id="using-a-persistentvolumeclaim-in-a-pod">Using a PersistentVolumeClaim in a Pod</h4>
<pre><code># kubectl create -f nfs-share-pod.yml
# kubectl get pods pod-nfs-share      # make sure pod is Running
# kubectl exec -it pod-nfs-share -- df -h /var/www
Filesystem                Size  Used Avail Use% Mounted on
192.168.43.48:/nfs_share   14G  8.6G  4.1G  68% /var/www
</code></pre>
<p>Recycle Persistent Volumes:</p>
<pre><code>k delete pod pod-nfs-share
k delete pvc nfs-share-pvc

[root@controller ~]# kubectl get pv     # Status - Released
NAME           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                         STORAGECLASS    REASON   AGE
nfs-share-pv   1Gi        ROX,RWX        Recycle          Released   default/nfs-share-pvc                                  80m
</code></pre>
<p>If we would have used <code>Retain</code> <code>ReclaimPolicy</code> then we would have to manually clean up the data in order to bind the <code>PersistentVolume</code> again.</p>
<h4 id="local-persistent-volumes-with-storageclass">Local Persistent Volumes with StorageClass</h4>
<pre><code>k create -f storage-class.yml 
k get sc    # check status

k create -f local-pv-sc.yml
k get pv
k describe pv local-pv

k create -f local-pvc.yml
k get pvc       # STATUS = Pending
k describe pvc local-storage-claim

k create -f local-pv-pod.yml
k get pods local-pod        # make sure it is `Running`
k get pvc                   # STATUS=Bound
k get pv                    # STATUS=Bound
</code></pre>
<h2 id="configmaps">ConfigMaps</h2>
<ul>
<li>Env-specfic data is provided to the application by the env it is deployed into.</li>
<li>ConfigMap defines application related data.</li>
</ul>
<p>Examples:</p>
<pre><code>kubectl create configmap my-config
    --from-file=foo.json            # single file
    --from-file=bar=foo.json        # single file stored under custom key
    --from-file=config-opts/        # dir
    --from-literal=foo=bar          # literal value

kubectl create cm nginx-cm --from-file nginx-custom-config.conf
k get cm

k create -f nginx-cm.yml
k exec -it nginx-cm -- cat /etc/nginx/conf.d/default.conf

# create CM using CLI args
k create cm myconfig --from-literal=color=red
k get cm

k create -f test-cm-pod.yml
k exec -it test-pod -- env | grep COLOR
</code></pre>
<h2 id="kubernetes-secrets">Kubernetes Secrets</h2>
<ul>
<li>Indented to store small amount (1 MB for a secret) of sensitive data.</li>
<li>A secret is base64 encoded, so we cannot treat it as secure.</li>
<li>K8s ensures that Secrets are passed only to the nodes that are running Pods that need the respective secrets.</li>
</ul>
<p>Examples:</p>
<pre><code>k create secret generic test-secret  --from-literal=user=deepak --from-literal=password=test1234
k get secrets test-secret -o yaml
echo dGVzdDEyMzQ= | base64 --decode

k create -f secret-busybox.yml
k exec -it secret-busybox -- /bin/sh
    # cat data/password
    # cat data/user
</code></pre>
<p>Defining secrets from a file:</p>
<pre><code>k create secret generic secret-tls --from=file=server.crt --from-file=server.key
k describe secret secret-tls

k create -f secret-ls-pod.yml
k get pods secret-tls-pod
k exec -it secret-ls-pod -- /bin/sh
    # ls -l /tls
</code></pre>
<h1 id="stateful-sets">Stateful sets</h1>
<ul>
<li>Available from k8s 1.5 as a bond between the Pod and the Persistent Volume.</li>
<li><strong>Pods names consist of name-I where I is zero based index.</strong></li>
<li>Replaced Pods get the same name and hostname as the Pod that has been
replaced.</li>
<li>Limitations:
<ul>
<li>storage for a given Pod must be provisioned by a PersistentVolume
Provisioner</li>
<li>deleting and/or scaling will not delete associated volumes.</li>
<li>Headless Service is required to be responsible for the network identity
of the Pods. You&rsquo;re responsible for creating this Service.</li>
<li>No guarantee on termination of pods on deletion. To achieve ordered and
graceful termination of the pods, scale the StatefulSet down to 0 prior
deletion.</li>
<li>When using Rolling Updates with the default Pod Management Policy
(OrderedReady), it&rsquo;s possible to get into a broken state that requires
manual intervention to repair.</li>
</ul>
</li>
</ul>
<p>Examples:</p>
<pre><code>[root@controller ~]# exportfs -v
/share1         (sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash)

k create -f nfs-pv-share1.yml -f nfs-pv-share2.yml -f nfs-pv-share3.yml
k api-resources | grep -iE 'KIND|stateful'
k explain StatefulSet | head -n 2

k create -f nfs-stateful.yml
k get statefulsets      # READY = 0/3
k get pvc               # STATUS = Bound
k get pv
k get pods              # no fancy names
k get pv                # all claimed

k get pods -o wide
k exec -it nginx-statefulset-2 -c nginx-statefulset -- touch /var/www/pod3-file
[root@controller ~]# ls -l /share3/
total 0
-rw-r--r-- 1 root root 0 Jan  9 16:44 pod3-file

k delete pod nginx-statefulset-2
k get pods -o wide      # new pod is being created with same IP and nodename
kubectl exec -it nginx-statefulset-2 -c nginx-statefulset -- ls -l /var/www/ # `pod3-file` is there
</code></pre>
<h1 id="k8s-api-server">K8s API Server</h1>
<ul>
<li>In k8s all communication between control plane and external clients (e.g. kubectl) are translated into REST API calls handled by the API server. The API server is the only component that talks directly with distributed storage etcd.</li>
<li>API server responsibilities is to provide k8s API and to proxy cluster components (e.g. dashboard), stream logs, service ports or serve kubectl exec sessions.</li>
<li>Master: etcd &lt;-&gt; API Server (Control Manager + Scheduler)</li>
<li>Worker: kubelet, native app</li>
<li>The API server is stateless and designed to scale horizontally. For HA it is recommended to have 3+ instances.</li>
</ul>
<p>Examples:</p>
<pre><code>k get pods -n kube-system
</code></pre>
<p><strong>K8s HTTP Request flow</strong>: client (kubectl) + Service Account -&gt; authentification -&gt; authorization -&gt; admission control -&gt; etcd.</p>
<p>After the request is authenticated and authorized, it goes to the admission control modules. K8s comes with predefined admission controllers, but you can define custom ones as well.</p>
<h2 id="service-account--roles">Service Account + Roles</h2>
<pre><code>k apply -f service-account.yml -f cluster-role.yml -f cluster-role-binding.yml

k get pods --all-namespaces
k describe pod -n kube-system kube-apiserver-minikube

# forbidden - returned by authorization plugin
k --as=system:serviceaccount:default:read-only-user get pods --all-namespaces
k --as=system:serviceaccount:default:read-only-user describe pod -n kube-apiserver-minikube

# inquiry 
k auth can-i get pods --all-namespaces  # yes
k --as=system:serviceaccount:default:read-only-user auth can-i get pods --all-namespaces

k delete serviceaccount read-only-user
k delete clusterrole read-only-user-cluster-role
</code></pre>
<p>HTTP Interface of the API Server:</p>
<ul>
<li>POST: k create -f</li>
<li>PUT: k apply -f</li>
<li>GET: k get; k describe</li>
<li>PATCH: k set image deployment/kuberserve nginx=nginx1.9.1</li>
<li>DELETE: k delete</li>
</ul>
<p>API resources and versions:</p>
<pre><code>k api-resources
k api-versions

k explain pod  # for any resource kind
</code></pre>
<p>K8s API via CLI:</p>
<pre><code>k get --raw /       # get all API resources
k get --raw /api/v1/namespaces  | jq
k get --raw /api/v1/namespaces/default  | jq

# access using curl
k proxy --port=9000
curl http://127.0.0.1:9000/apis | less
curl http://127.0.0.1:9000/apis/apps/v1/namespaces/default/deployments
</code></pre>
<h2 id="containers-security-context">Container&rsquo;s Security Context</h2>
<ul>
<li>Each pod gets its own IP and port space. Each pod has its own process tree, its own IPC namespace, allowing only processes in the same pod to communicate through IPC.</li>
<li>Here we learn to allow pods to access resources of the node they&rsquo;re running on.</li>
</ul>
<p>Example:</p>
<pre><code>k create -f pod-with-host-network.yml
k get pods
k exec pod-with-host-network -- ifconfig

# Allow pods to bind a port in the node's default namespace 
# using `ports.hostPort`
# (only one instance of the pod can be schedules to each node)
k create -f nginx-lab.yml
curl 127.0.0.1:9000     # XXX doesn't work (because of minikube?)

# Using node's PID and IPC namespaces
k create -f pod-with-host-pid-and-ipc.yml
k exec pod-with-host-pid-and-ipc -- ps aux  # includes container process
</code></pre>
<h2 id="configure-containers-security-context">Configure container&rsquo;s security context</h2>
<pre><code># run with given user
$ k run pod-with-defaults --image alpine --restart Never -- /bin/sleep 99999
$ k exec pod-with-defaults -- id
uid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video)
$ k create -f pod-as-user-guest.yml 
$ k exec pod-as-user-guest -- id
uid=65534(nobody) gid=65534(nobody)

# run as non-root
k create -f pod-run-as-non-root.yml
k get po pod-run-as-non-root        # STATUS=CreateContainerConfigError
k describe pods pod-run-as-non-root # Error: container has runAsNonRoot and image will run as root

# run in privileged mode (to get full access to node's kernel)
k create -f pod-privileged.yml
k exec -it pod-with-defaults -- ls /dev     # 16
k exec -it pod-privileged -- ls /dev     # 241
</code></pre>
<h2 id="individual-capabilities">Individual Capabilities</h2>
<pre><code># add individual capabilities to a container
k exec -it pod-with-defaults -- date +%T -s &quot;12:00:00&quot;      # date: can't set date: Operation not permitted
k create -f pod-add-settime-capability.yml
k exec -it pod-add-settime-capability -- date +%T -s &quot;12:00:00&quot;; date

# drop individual capabilities from a container
k create -f pod-drop-chown-capability.yml 
k exec pod-drop-chown-capability -- chown guest /tmp    # operation not permitted

# ro FS
k create -f pod-with-readonly-filesystem.yml
k exec -it pod-with-readonly-filesystem -- touch /new-file  # exit 1 - Read-only file system

k exec -it pod-with-readonly-filesystem -- touch /volume/newfile    # works
k exec -it pod-with-readonly-filesystem -- ls -la /volume/newfile
</code></pre>
<h1 id="authentication">Authentication</h1>
<ul>
<li>Client certificates (most common) using X509 CA. <code>--client-ca-file=file_path</code> server option.</li>
<li>Static tokens: ``-token-auth-file=<code>. Tokens persist indefinitely and the API server needs to be restarted to update the tokens. Username and password are passed in the request header: </code>Authentication: Basic base64(user:password)`</li>
<li>Bootstrap tokens: are dynamically managed and stored as secrets in <code>kube-system</code>. <code>--enable-bootstrap-token-auth</code> option for the CLI server.</li>
<li>Service account tokens: <code>--service-acount-key-file</code></li>
<li>Authentication proxy: <code>--requestheader-{username-headers,group-headers,extra-headers-prefix}</code> arguments.</li>
<li>Webhook tokens: <code>authorization-webhook-config-file=</code></li>
</ul>
<p><code>kubectl</code> uses certificates stores in <code>~/.kube/config</code> or <code>/etc/kubernetes/admin.conf</code>.</p>
<p>Example using client certificates:</p>
<pre><code>[root@controller ~]# kubectl config view | grep server
server: https://192.168.43.48:6443

[root@controller ~]# curl https://192.168.43.48:6443
curl: (60) SSL certificate problem: unable to get local issuer certificate

[root@controller ~]# export client=$(grep client-cert /etc/kubernetes/admin.conf | cut -d &quot; &quot; -f 6)
[root@controller ~]# export key=$(grep client-key-data /etc/kubernetes/admin.conf | cut -d &quot; &quot; -f 6)
[root@controller ~]# export auth=$(grep certificate-authority-data /etc/kubernetes/admin.conf | cut -d &quot; &quot; -f 6)

[root@controller ~]# echo $client | base64 -d - &gt; client.pem
[root@controller ~]# echo $key | base64 -d - &gt; client-key.pem
[root@controller ~]# echo $auth | base64 -d - &gt; ca.pem

[root@controller ~]# curl --cert client.pem --key client-key.pem --cacert ca.pem  https://192.168.43.48:6443   # works 
</code></pre>
<h1 id="authorization">Authorization</h1>
<ul>
<li>Node. Enabled by default.</li>
<li>ABAC: requests are validating policies against the attributes of the request. <code>--authorization-policy-file=</code> and <code>--authorization-mode=ABAC</code> options.</li>
<li>RBAC. To enable start the API server with <code>--authorization-mode=RBAC</code>.</li>
<li>Webhooks (uses remote API server to check for permissions). Option <code>--authorization-webhook-config-file=</code>.</li>
</ul>
<h2 id="rbac">RBAC</h2>
<p>kubeconfig:</p>
<ul>
<li>users: username and authentication mechanism</li>
<li>clusters: all data necessary to connect to the cluster</li>
<li>contexts: association between users and clusters</li>
</ul>
<p>Create user example:</p>
<pre><code># create linux user
useradd -G nogroup user1
passwd user1

# create certs
openssl genrsa -out user1.key 4096
openssl req -new -key user1.key -out user1.csr -subj &quot;/CN=user1/O=dev&quot;  # cert. signing request
# openssl x509 -req -in user1.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out user1.crt -days 365
openssl x509 -req -in user1.csr -CA ~/.minikube/ca.crt -CAkey ~/.minikube/ca.key -CAcreateserial -out user1.crt -days 365

# create namespace (optional)
k create namespace dev

# update k8s config with user credentials
k config view
k config set-credentials user1 --client-certificate=user1.crt --client-key=user1.key
k config view

# create security context for new user
k config set-context user1-context --cluster=kubernetes --namespace=dev --user=user1    # and set default namespace
k config get-contexts
k --context=user1-context get pods  # XXX failed (minikube?)
</code></pre>
<p>Define new role with &ldquo;modify&rdquo; permission:</p>
<pre><code>kubectl api-resources | grep -iE 'role|KIND'
k explain Role | head -n 2

k create -f dev-role.yml
k create -f user1-rolebind.yml 

k --context=user1-context get pods      # XXX failed (minikube?)

k -n dev describe role dev

k create deployment devnginx --image=nginx --context=user1-context 
k get pods          # not there
k get pods -n dev   # but here

# testing
k auth can-i create pods --context=user1-context    # yes
k auth can-i create service --context=user1-context    # no

# Define role with &quot;view-only&quot; permission
k create namespace view-only
k create -f view-only-role.yml
k create -f view-only-rolebinding.yml

k config set-context viewonly-context --cluster=kubernetes --namespace=view-only --user=user1
k config get-contexts

k --context=viewonly-context get pods
k -n view-only describe role view-only
k create deployment testnginx --image=nginx --context=viewonly-context    # FORBIDDEN

k auth can-i create pods --context=viewonly-context     # no
k auth can-i get pods --context=viewonly-context        # yes
k auth can-i get service --context=viewonly-context     # no
</code></pre>
<p>Deleting Context, Role, RoleBinding:</p>
<pre><code>k config delete-context user1-context
k config delete-context viewonly-context

k delete role dev -n dev
k delete rolebinding dev-role-binding -n dev
</code></pre>
<h1 id="limit-resources">Limit Resources</h1>
<ul>
<li>Resource quota is applied on the namespace.</li>
<li>Resource limit is applied on the containers.</li>
<li>If creating or updating a resource violates a quota contraint -&gt; HTTP 403.</li>
<li>Is quota enabled in a namespace for compute resources (cpu, mem), users must specify requests or limits for those value, otherwise quota system may reject pod creation.</li>
</ul>
<h2 id="resource-quota-types">Resource quota types</h2>
<p>Compute resources:</p>
<ul>
<li><strong>limits.cpu</strong>: sum of CPU limits cannot exceed this value</li>
<li><strong>limits.memory</strong>: sum of memory limits cannot exceed this value</li>
<li><strong>requests.cpu</strong>: dtto for requests</li>
<li><strong>requests.memory</strong>: dtto for requests</li>
</ul>
<p>Storage resource quota:</p>
<ul>
<li><strong>requests.storage</strong>: total amount of requested storage across all persistent volume claims</li>
<li><strong>persistentvolumeclaims</strong>: maximum number of persistent volume claims allowed in the namespace</li>
<li><strong>.storageclass.storage.k8s.io/requests.storage</strong>: total amount of requested storage across all persistent volume claims associated wit the storage class name</li>
<li><strong>.storageclass.storage.k8s.io/persistentvolumeclaims</strong>: maximum number of persistent volume claims allowed in the namespace that are associated with the storage class name</li>
<li><strong>requests.ephemeral-storage</strong>: total amount of requested ephemeral storage across all pods in the namespace claims</li>
<li><strong>limits.ephemeral-storage</strong>: total amount of limits for empeheral storage across all pods in the namespace claims.</li>
</ul>
<p>Object count quota:</p>
<ul>
<li><strong>count/.</strong> for resources from non-core groups</li>
<li><strong>count/</strong> for resources from the core group</li>
</ul>
<p>Some of these:</p>
<pre><code>count/persistentvolumeclaims
count/services
count/secrets
count/configmaps
count/replicationcontrollers
count/deployments.apps
count/replicasets.apps
count/statefulsets.apps
count/jobs.batch
count/cronjobs.batch
</code></pre>
<p>Example:</p>
<pre><code>k create namespace quota-example
k apply -f ns-quota-limit.yml 
k describe ns quota-example
k create -f pod-nginx-lab-1.yml
k -n quota-example scale deployment/example --replicas=5
k -n quota-example get events
k -n quota-example get pods     # ready
k describe ns quota-example     # show current

k delete deployments -n quota-example example
k delete ns quota-example

# count quota for pods
k create ns pods-quota-ns
k apply -f pod-quota-limit.yml
k get resourcequota -n pods-quota-ns        # REQUEST pods: 0/2
k describe ns pods-quota-ns

k create -f nginx-example.yml 
k get pods -n pods-quota-ns                 # READY 1/1
k -n pods-quota-ns scale deployment/nginx-1 --replicas=5
k get pods -n pods-quota-ns                 # 2
k -n pods-quota-ns get events               # FailedCreate ...
k describe ns pods-quota-ns                 # 2/2
</code></pre>
<h2 id="limit-range">Limit Range</h2>
<p>If a LimitRange object exists in a namespace, then any container created without the resource requests or limits configured will inherit these values from the limit range.</p>
<p>Example:</p>
<pre><code>k create -f assign-limit-range.yml
k get limits -n pods-quota-ns           # define-limit created $NOW
k describe ns pods-quota-ns
k describe limits -n pods-quota-ns define-limit     # only the limits

k delete deployments.apps -n pods-quota-ns nginx-1 
k get pods -n pods-quota-ns                 # 0

k apply -f nginx-example.yml
k describe pods -n pods-quota-ns nginx-1        # requests/limits inherited

k delete limitrange -n pods-quota-ns define-limit
</code></pre>
<h2 id="limiting-resources">Limiting resources</h2>
<pre><code>spec.containers[].resources.limits.cpu
spec.containers[].resources.limits.memory
spec.containers[].resources.limits.hugepages-
spec.containers[].resources.requests.cpu
spec.containers[].resources.requests.memory
spec.containers[].resources.requests.hugepages-
</code></pre>
<p>CPU:</p>
<ul>
<li>CPU unit is 1 core for cloud providers and 1 hyperthread for bare metal Intel processors</li>
<li>.5 is half a core, or 500 m (mili-cores).</li>
<li>Smallest addressable unit is 1m.</li>
</ul>
<p>Memory:</p>
<ul>
<li>K/M/G/T/P/E suffix - 1000**N</li>
<li>Ki/Mi/Gi/Ti/Pi/Ei - 1024**N</li>
</ul>
<p>Example:</p>
<pre><code>k create ns cpu-limit
k get ns
k create -f pod-resource-limit.yml
k get pods -n cpu-limit -o wide
k get pods -o wide
k describe pods frontend

k delete pods -n cpu-limit frontend
</code></pre>
<p>If no CPU limit is specified and there is no <code>LimitRange</code> object, the countainer will use all the available CPU resources on the node it is running.</p>
<h1 id="expose-containers-to-external-networks">Expose containers to external networks</h1>
<h2 id="kubectl-port-forwarding">kubectl port forwarding</h2>
<pre><code>kubectl port-forward nginx 8888:80
curl localhost:8888
</code></pre>
<h2 id="kubectl-expose">kubectl expose</h2>
<pre><code>k create deployment nginx-lab-1 --image=nginx --replicas=3 --dry-run=client -o yaml &gt; nginx-lab-1.yml
k create -f nginx-lab-1.yml
k get pods | grep nginx-lab-1

k expose deployment nginx-lab-1 --type=NodePort --port=80
k get services
# ...
# nginx-lab-1   NodePort    10.108.128.242   &lt;none&gt;        80:31060/TCP   3s
k describe svc nginx-lab-1  # to get more info about the service
</code></pre>
<h2 id="ingress">Ingress</h2>
<ul>
<li>provide an externally visible URL to the service</li>
<li>load balance traffic</li>
<li>terminates SSL</li>
<li>provide name-based virtual hosting</li>
</ul>
<p>Only creating an Ingress resource has no efect. You need to select and deploy one to your cluster (many implementations, e.g. nginx or HAProxy).</p>
<p><a href="Examples">Examples</a>:</p>
<pre><code>minikube version
kubectl get nodes

minikube addons enable ingress
k get pods -n kube-system   # ingress-nginx-*
</code></pre>
<h3 id="configure-ingress-using-host">Configure Ingress using Host</h3>
<pre><code>k create deployment nginx --image=nginx
k scale deployment nginx --replicas=3
k get deployments

k expose deployment nginx --type=NodePort --port=80
k get service
minikube service nginx --url        # or `ip a`
# add this to /etc/hosts as host.example.com
k create -f nginx-ingress-rule.yml
k get ingress
k get ing nginx-ingress -o yam
curl http://host.example.com
</code></pre>
<h3 id="configure-ingress-using-path">Configure Ingress using Path</h3>
<pre><code>k create deployment web2 --image=nginx
k scale deployment web2 --replicas=3
k expose deployment web2 --type=NodePort --port=80
k get svc   # +web2
minikube service web2 --url
# vi nginx-ingress-rule.yml
k apply -f nginx-ingress-rule.yml
k get ing nginx-ingress -o yaml
curl http://host.example.com/v2
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->

    </div>
  </article>

  
  




  <div class="blog-post-comments">
    
      <div id="cactus-comments-thread">
  <script>
  initComments({
    node: document.getElementById("cactus-comments-thread"),
    defaultHomeserverUrl: 'https:\/\/matrix.cactus.chat:8448',
    serverName: 'cactus.chat',
    siteName: "your_cactus_comments_sitename",
    commentSectionId: "\/posts\/2021-02-22-kubernetes\/"
  })
</script>
</div>

    
  </div>



  <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/posts">Blog</a></li>
         
          <li><a href="/tags">Tags</a></li>
         
          <li><a href="/etymolog">Etymolog</a></li>
         
          <li><a href="/sask.pdf">SA-SK</a></li>
        
      </ul>
    </div>

    

    <div id="share-footer" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f" aria-label="Facebook">
      <i class="fab fa-facebook fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f&text=Kubernetes" aria-label="Twitter">
      <i class="fab fa-twitter fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f&title=Kubernetes" aria-label="Linkedin">
      <i class="fab fa-linkedin fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f&is_video=false&description=Kubernetes" aria-label="Pinterest">
      <i class="fab fa-pinterest fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=Kubernetes&body=Check out this article: https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f" aria-label="Email">
      <i class="fas fa-envelope fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f&title=Kubernetes" aria-label="Pocket">
      <i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f&title=Kubernetes" aria-label="reddit">
      <i class="fab fa-reddit fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f&name=Kubernetes&description=source%0aInstall%20and%20Deploy%20%20install%20kubectl%20install%20minikube%20%20Commands%3a%0aminikube%20start%20minikube%20status%20kubectl%20get%20node%20kubectl%20get%20pods%20kubectl%20create%20deployment%20hello-minikube%20--image%3dk8s.gcr.io%2fechoserver%3a1.4%20kubectl%20expose%20deployment%20hello-minikube%20--type%3dNodePort%20--port%3d8080%20minikube%20service%20hello-minikube%20--url%20curl%20http%3a%2f%2f192.168.49.2%3a31223%20kubectl%20delete%20deployments.apps%20hello-minikube%20minikube%20pause%20minikube%20unpause%20minikube%20stop%20%20Namespaces%20%20for%20organization%20and%20resource%20separation%20kubectl%20--namespace%3dmystuff%20or%20kubectl%20-n%3dmystuff%20kubectl%20--all-namespaces%20%20Default%20namespaces%20for%20new%20clusters%3a%0a%24%20kubectl%20get%20ns%20NAME%20STATUS%20AGE%20default%20Active%2013m%20%23%20k8s%20resources%20are%20crated%20here%20by%20default%20kube-node-lease%20Active%2013m%20%23%20storage%20for%20node%20lease%20information%20kube-public%20Active%2013m%20%23%20world-readable%20kube-system%20Active%2013m%20%23%20infrastructure%20pods%20%20Commands%3a" aria-label="Tumblr">
      <i class="fab fa-tumblr fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fuzak.github.io%2fposts%2f2021-02-22-kubernetes%2f&t=Kubernetes" aria-label="Hacker News">
      <i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>

    <div id="actions-footer">
      
        <a id="menu-toggle" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;" aria-label="Menu">
          <i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
        <a id="share-toggle" class="icon" href="#" onclick="$('#share-footer').toggle();return false;" aria-label="Share">
          <i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" aria-label="Top of Page">
          <i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>


  <footer id="footer">
  <div class="footer-left">
    Copyright  &copy; 2022  me 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/posts">Blog</a></li>
         
        <li><a href="/tags">Tags</a></li>
         
        <li><a href="/etymolog">Etymolog</a></li>
         
        <li><a href="/sask.pdf">SA-SK</a></li>
        
      </ul>
    </nav>
  </div>
</footer>


  </div>
</body>

<link rel="stylesheet" href=/lib/font-awesome/css/all.min.css>
<script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>

<script src=/js/code-copy.js></script>




</html>
